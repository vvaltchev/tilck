#!/usr/bin/env python3
# SPDX-License-Identifier: BSD-2-Clause
# pylint: disable=unused-wildcard-import, bad-indentation, wildcard-import
# pylint: disable=missing-function-docstring, wrong-import-position
# pylint: disable=invalid-name, raise-missing-from, missing-class-docstring
# pylint: disable=consider-using-f-string

import os
import re
import sys
import time
import argparse
import subprocess
import ctypes
import concurrent.futures as ft
from typing import List, Dict, Tuple

# Constants coming from CMake (this file gets pre-processed by CMake)
RUNNERS_DIR = '@CMAKE_SOURCE_DIR@/tests/runners'
SHELLCMDS_TABLE = '@CMAKE_SOURCE_DIR@/tests/system/cmds_table.h'
KERNEL_FILE = '@KERNEL_FILE@'
BUILD_DIR = '@CMAKE_BINARY_DIR@'
KERNEL_FORCE_TC_ISYSTEM = '@KERNEL_FORCE_TC_ISYSTEM@'
ELFHACK = '@CMAKE_BINARY_DIR@/scripts/build_apps/elfhack@ARCH_BITS@'
ARCH_BITS = int('@ARCH_BITS@')

sys.path.append(RUNNERS_DIR)
from lib.utils import *
from lib.detect_kvm import *
from lib.env import *
from lib.exceptions import *

# Constants

# Placeholder timeout value for manual tests, more convenient than 0 for sorting
MANUAL_TEST_TIMEOUT = 10000

# Must match with `enum se_kind` in self_tests.h
TIMEOUTS = {
   'manual' : MANUAL_TEST_TIMEOUT,
   'short': 12,
   'med': 36,
   'medlong': 72,
   'long': 108,
}


TIMEOUTS_BY_KIND_NUM = list(TIMEOUTS.values())
INV_TIMEOUTS = { TIMEOUTS[k]: k for k in TIMEOUTS }
INF_STR_TIMEOUTS = ['any', 'all', 'inf']

ordered_timeouts = sorted(list(INV_TIMEOUTS.keys()))

# Timeout used for the `runall` shellcmd (-c option)
ALL_TESTS_TIMEOUT   = 4 * TIMEOUTS['med']

load_tests_func_by_type_list = [
   'load_list_of_kernel_self_tests',
   'load_list_of_shell_cmd_tests',
   'load_list_of_interactive_tests',
]

tt_pretty_names = {
   val: TEST_TYPES_PRETTY[i] for i, val in enumerate(TEST_TYPES)
}

load_tests_func_by_type = {
   val: load_tests_func_by_type_list[i] for i, val in enumerate(TEST_TYPES)
}

tt_indexes = {
   val : i for i, val in enumerate(TEST_TYPES)
}

# Global variables

tests_by_type : Dict[str, List[Tuple[str,int]]] = { k: [] for k in TEST_TYPES }
tests_to_run  : Dict[str, int]                  = { k:  0 for k in TEST_TYPES }
tests_passed  : Dict[str, int]                  = { k:  0 for k in TEST_TYPES }
test_runners  : Dict[str, str]                  = { k: "" for k in TEST_TYPES }

def timeout_name(val):

   if val in INV_TIMEOUTS:
      return INV_TIMEOUTS[val]

   for t in ordered_timeouts:

      if val <= t:
         return INV_TIMEOUTS[t]

   return "(other)"

def arg_type_test(s):

   if s in TEST_TYPES:
      return s

   tt = list(filter(lambda x: x.startswith(s), TEST_TYPES))

   if len(tt) == 0:
      raise argparse.ArgumentTypeError("No such test type: '{}'".format(s))

   if len(tt) > 1:
      raise argparse.ArgumentTypeError("Ambiguous test type: '{}'".format(s))

   return tt[0]

def arg_type_timeout(s):

   if s in TIMEOUTS:
      return TIMEOUTS[s]

   if s in INF_STR_TIMEOUTS:
      return 0

   # OK, at this point 's' must be an integer

   try:

      return int(s)

   except:

      raise argparse.ArgumentTypeError(
         "Timeout must be an integer or one of: {}"
            .format(", ".join(list(TIMEOUTS.keys()) + INF_STR_TIMEOUTS))
      )

def load_all_tests():

   for tt in TEST_TYPES:
      tests_by_type[tt] = globals()[load_tests_func_by_type[tt]]()

def load_list_of_interactive_tests():

   result = []
   rit = os.path.join(BUILD_DIR, 'st', 'run_interactive_test')

   try:

      output = subprocess.check_output([rit, '-l']).decode("utf-8")

   except subprocess.CalledProcessError as e:

      if e.returncode == Fail.invalid_build_config.value:
         # That's perfectly fine: this build has not been configured for
         # running interactive tests. Just return an empty list.
         return []

      if e.returncode == Fail.invalid_system_config.value:
         # That's also possible (i.e. ImageMagick is not installed). Return
         # an empty list as above
         return []

      # Oops, something weird happened: we cannot ignore the error.
      raise

   arr = output.split("\n")

   for r in arr:

      r = r.strip()

      if r:
         result.append([r, TIMEOUTS['medlong']])

   test_runners["interactive"] = "@CMAKE_BINARY_DIR@/st/run_interactive_test"
   return result

def load_list_of_shell_cmd_tests():

   result = []

   with open(SHELLCMDS_TABLE, 'r') as fh:

      lines = fh.readlines()

      for line in lines:

         line = line.strip()
         m = re.match(r'CMD_ENTRY\((\w+),\s*(\w+),\s*(false|true)\)', line)

         if not m:
            continue

         name = m[1]
         timeout_str = m[2].lower()
         enabled_str = m[3]

         if enabled_str == 'true':
            if timeout_str.startswith("tt_"):
               t = timeout_str[3:]
               result.append([name, TIMEOUTS[t]])

   test_runners["shellcmd"] = "@CMAKE_BINARY_DIR@/st/single_test_run"
   return sorted(result, key = lambda x: x[1])

class kernel_selftest_struct(ctypes.Structure):
   _fields_ = [
      ("node", ctypes.c_char * 8 * (ARCH_BITS // 32)),
      ("name", ctypes.c_char * 4 * (ARCH_BITS // 32)),
      ("func", ctypes.c_char * 4 * (ARCH_BITS // 32)),
      ("kind", ctypes.c_int),
   ]

def get_sym_data(sym):

   raw = subprocess                                                      \
            .check_output([ELFHACK, KERNEL_FILE, '--dump-sym', sym])     \
               .decode('utf-8')

   raw_arr = raw.split(' ')[:-1] # skip the last '\n' char
   data = [ int('0x' + x, 16) for x in raw_arr ]
   return bytes(data)

def get_selftest_obj(sym):
   return kernel_selftest_struct.from_buffer_copy(get_sym_data(sym))

def load_list_of_kernel_self_tests():

   result = []
   rows = subprocess                             \
            .check_output(['nm', KERNEL_FILE])   \
               .decode("utf-8")                  \
                  .split("\n")

   for row in rows:

      row = row.rstrip()

      if not row:
         continue

      _, _, name = row.split(' ')
      m = re.match(r'^se_(.+)_inst$', name)

      if not m:
         continue

      sym_name = name
      test_name = m.group(1)
      obj = get_selftest_obj(sym_name)
      tt = TIMEOUTS_BY_KIND_NUM[obj.kind]
      result.append([test_name, tt])

   test_runners["selftest"] = "@CMAKE_BINARY_DIR@/st/single_test_run"
   return sorted(result, key = lambda x: x[1])


def internal_single_test_runner_thread(test_type : str,
                                       test : str,
                                       timeout : int,
                                       show_output : bool):

   raw_print(
      "[ RUNNING ] {}: '{}' [timeout: {}]".format(test_type, test, timeout)
   )

   cmdline = [
      test_runners[test_type],
      test_type,
      test,
      str(timeout),
      get_qemu_kvm_version(),
   ]

   start_time = time.time()

   p = subprocess.Popen(
      cmdline,
      stdin = subprocess.DEVNULL,
      stdout = subprocess.PIPE,
      stderr = subprocess.STDOUT,
   )

   if show_output:
     raw_print("")

   bin_output = b''

   while p.poll() is None:

      bintext = p.stdout.read()

      if not bintext:
         time.sleep(0.1)
         continue

      if show_output:
         direct_print(bintext)
      else:
         bin_output += bintext

   elapsed = time.time() - start_time

   if p.returncode == Fail.success.value:

      raw_print("[ PASSED  ] after {:.2f} seconds\n".format(elapsed))

   else:

      if not show_output:
         raw_print("")
         direct_print(bin_output)

      raw_print(
         "[ FAILED  ] after {:.2f} seconds with: {}\n"
            .format(elapsed, get_fail_by_code(p.returncode))
      )

   return p.returncode

def internal_run_test(test_type : str,
                      test : str,
                      timeout : int,
                      show_output : bool = False):

   with ft.ThreadPoolExecutor(max_workers = 1) as executor:

      future = executor.submit(
         internal_single_test_runner_thread,
         test_type,
         test,
         timeout,
         show_output
      )

      return future.result()

def does_test_match_criteria(x : Tuple[str,int],
                             reg : str,
                             max_timeout : int,
                             allow_manual : bool):

   name : str = x[0]
   timeout : int = x[1]
   is_runall = name in ['runall', 'runall_manual']
   is_manual = timeout == MANUAL_TEST_TIMEOUT
   timeout_check = timeout <= max_timeout
   manual_check = not is_manual or allow_manual
   matches_filter = re.match(reg, name) and (timeout_check or manual_check)

   return is_runall or (matches_filter and manual_check)

def list_tests(reg : str, max_timeout : int, test_type : str):

   col_names : List[Tuple[str, int]] = [
      ('test name', 30),
      ('test type', 12),
      ('timeout', 10),
   ]
   raw_stdout_write("\n")

   for x in col_names:
      raw_stdout_write('+-')
      raw_stdout_write(''.center(x[1], '-'))
   raw_stdout_write('+\n')

   for x in col_names:
      raw_stdout_write('| ')
      raw_stdout_write(x[0].center(x[1], ' '))
   raw_stdout_write('|\n')

   for x in col_names:
      raw_stdout_write('+-')
      raw_stdout_write(''.center(x[1], '-'))
   raw_stdout_write('+\n')

   for tt in TEST_TYPES:

      count = 0

      if test_type and tt != test_type:
         continue

      for x in tests_by_type[tt]:
         if does_test_match_criteria(x, reg, max_timeout, True):

            tval = x[1]

            if tval != MANUAL_TEST_TIMEOUT:
               t = "{}".format(tval)
            else:
               t = "<manual>"

            raw_stdout_write('| ')
            raw_stdout_write(x[0].ljust(col_names[0][1]))
            raw_stdout_write('| ')
            raw_stdout_write(tt.ljust(col_names[1][1]))
            raw_stdout_write('| ')
            raw_stdout_write(t.ljust(col_names[2][1]))
            raw_stdout_write('|\n')
            count += 1

      if count > 0:
         for x in col_names:
            raw_stdout_write('+-')
            raw_stdout_write(''.center(x[1], '-'))
         raw_stdout_write('+\n')

   raw_stdout_write('\n')

def get_sum(per_test_counter):
   return sum(per_test_counter[k] for k in per_test_counter)

def run_test(test_type : str,
             x : Tuple[str,int],  # test, timeout
             show_output : bool):

   if get_sum(tests_to_run) == 0:
      raw_print("")

   tests_to_run[test_type] += 1
   ret = internal_run_test(test_type, x[0], x[1], show_output)

   if ret == Fail.success.value:

      tests_passed[test_type] += 1

   elif ret == Fail.shell_unknown_exit_code.value:

      # From time to time, the per-second message by the "sched alive thread"
      # interleaves the shell's exit code like that:
      #
      # [init] the shell with [    2.139] ---- Sched alive thread: 2 ----
      # pid 2 exited with status: 0
      #
      # And this prevents the runner to correctly parse shell's exit code,
      # resulting in a false positive failure.
      #
      # To mitigate this problem, let's re-run the test just once, if it has
      # failed with: shell_unknown_exit_code.

      ret = internal_run_test(test_type, x[0], x[1], show_output)

      if ret == Fail.success.value:
         tests_passed[test_type] += 1


def run_all_tests(max_timeout : int,
                  show_output : bool,
                  reg : str,
                  fail_on_zero : bool,
                  test_type : str,
                  allow_manual : bool):

   for tt in TEST_TYPES:

      if test_type and tt != test_type:
         continue

      for x in tests_by_type[tt]:
         if does_test_match_criteria(x, reg, max_timeout, allow_manual):
            run_test(tt, x, show_output)

   if fail_on_zero:
      if sum(tests_to_run.values()) == 0:

         found = []
         for tt in TEST_TYPES:
            for x in tests_by_type[tt]:
               if re.match(reg, x[0]):
                  found.append(x)

         if not found:
            raw_print("ERROR: No tests matching the '{}' regex.".format(reg))
         else:
            raw_print(
               "ERROR: No tests matching "
               "the '{}' regex having timeout <= {}"
                  .format(reg, max_timeout)
            )
            raw_print(
               "Tests matching the regex with timeout > {}:"
                  .format(max_timeout)
            )
            for x in found:
               if x[1] != MANUAL_TEST_TIMEOUT:
                  raw_print("  {} [timeout: {}s]".format(x[0].ljust(20), x[1]))
               else:
                  raw_print("  {} [manual test]".format(x[0].ljust(20)))

         sys.exit(Fail.no_tests_matching.value)

def dump_test_stats():

   raw_print('-' * 80)

   for tt in TEST_TYPES:
      if tests_to_run[tt]:
         raw_print("{} passed: {}/{}"
                     .format(tt_pretty_names[tt],
                             tests_passed[tt],
                             tests_to_run[tt]))

def parse_args():

   parser = argparse.ArgumentParser()

   g = parser.add_mutually_exclusive_group()

   g.add_argument(
      "-l", "--list-tests",
      action = "store_true",
      help = "List all tests matching the criteria (dry-run)"
   )

   g.add_argument(
      "-L", "--list-all-tests",
      action = "store_true",
      help = "List all tests, no matter their timeout (same as -l -t any)"
   )

   g.add_argument(
      "-j", "--list-timeouts",
      action = "store_true",
      help = "List the predefined timeout values with labels usable with -t"
   )

   parser.add_argument(
      "-c", "--compact-run",
      action = "store_true",
      help = "Run all the shellcmds in a single VM run (faster)"
   )

   parser.add_argument(
      "-o", "--show-output",
      action = "store_true",
      help = "Show test's output even in case of success"
   )

   parser.add_argument(
      "-a", "--allow-manual",
      action = "store_true",
      help = "Allow manual tests to be run by this script. "
             "Warning: some manual tests make the kernel to panic and that "
             "will be interpreted as a failure by this test runner."
   )
   parser.add_argument(
      "-f", "--filter",
      type = str,
      default = ".*",
      help = "Run only tests matching the given regex filter"
   )

   parser.add_argument(
      "-t", "--max-timeout",
      type = arg_type_timeout,
      default = TIMEOUTS['med'],
      help = "Run only tests having timeout <= MAX_TIMEOUT "
             "(default: {})".format(TIMEOUTS['med'])
   )

   parser.add_argument(
      "-T", "--test_type",
      type = arg_type_test,
      help = "Run only tests of the given type. Minimal prefixes are "
             "accepted (e.g. 'se' is interpreted as selftest)."
   )

   try:
      args = parser.parse_args()
   except SystemExit:
      sys.exit(Fail.invalid_args.value)

   return args

def list_timeouts():

   for tname, tval in TIMEOUTS.items():
      if tname != 'manual':
         raw_print("{:8}: {:3} seconds".format(tname, tval))

   for s in INF_STR_TIMEOUTS:
      raw_print("{:8}: <no timeout>".format(s))

def main():

   set_runner_name("test runner")
   load_all_tests()
   args = parse_args()

   if args.compact_run:
      tests_by_type['shellcmd'] = [ ['runall', ALL_TESTS_TIMEOUT] ]

   if args.list_timeouts:
      list_timeouts()
      sys.exit(0)

   if args.list_all_tests:
      args.max_timeout = MANUAL_TEST_TIMEOUT
      args.list_tests = True

   if args.max_timeout == 0:
      args.max_timeout = MANUAL_TEST_TIMEOUT

   if args.list_tests:
      list_tests(args.filter, args.max_timeout, args.test_type)
      sys.exit(0)

   detect_kvm()

   if is_cmake_opt_enabled(KERNEL_FORCE_TC_ISYSTEM):
      unrunnable_build_graceful_exit()

   try:

      run_all_tests(args.max_timeout,
                    args.show_output,
                    args.filter,
                    not args.compact_run,
                    args.test_type,
                    args.allow_manual)

   except KeyboardInterrupt:
      msg_print("KeyboardInterrupt")
      sys.exit(Fail.user_interruption.value)

   dump_test_stats()

   if get_sum(tests_passed) != get_sum(tests_to_run):
      sys.exit(Fail.some_tests_failed.value)

###############################
if __name__ == '__main__':
   main()
